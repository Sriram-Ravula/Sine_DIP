{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "import torch\n",
    "import torchvision as tv\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#import torch_dip_utils as utils\n",
    "import utils\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Hyperparameters, network filter and I/O sizes, and waveform parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up hyperparameters, net input/output sizes, and whether the problem is compressed sensing\n",
    "\n",
    "LR = 1e-3 # learning rate\n",
    "MOM = 0.9 # momentum\n",
    "NUM_ITER = 100 # number iterations\n",
    "WD = 1e-4 # weight decay for l2-regularization\n",
    "\n",
    "Z_NUM = 32 # input seed\n",
    "NGF = 64 # number of filters per layer\n",
    "ALEX_BATCH_SIZE = 1 # batch size of gradient step\n",
    "nc = 1 #num channels in the net I/0\n",
    "\n",
    "#choose the number of samples and periods in the training waveform\n",
    "WAVE_SIZE = 1024\n",
    "WAVE_PERIODS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose whether the problem is compressed sensing or DIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed = False\n",
    "\n",
    "if compressed:\n",
    "    num_measurements = 64\n",
    "else:\n",
    "    num_measurements = WAVE_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use CUDA if Possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "<torch.cuda.device object at 0x7f7600547128>\n"
     ]
    }
   ],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "print(CUDA)\n",
    "\n",
    "#save the correct datatype depending on CPU or GPU execution\n",
    "if CUDA : \n",
    "    dtype = torch.cuda.FloatTensor  \n",
    "    print(torch.cuda.device(0))\n",
    "else:\n",
    "    dtype = torch.FloatTensor\n",
    "    print(\"NO DEVICES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and plot the training and reference waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Produces a sinusoid with optional additive gaussian noise distributed (mean, std)\n",
    "def get_sinusoid(num_samples, num_periods, noisy=True, std=0.1, mean=0):\n",
    "    \n",
    "    Fs = num_samples\n",
    "    x = np.arange(num_samples)\n",
    "    \n",
    "    y = np.sin(2*np.pi * num_periods * x / Fs)\n",
    "    \n",
    "    if noisy:\n",
    "        y += (std * np.random.randn(num_samples)) + mean\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util function for normalizing noisy wave range to [-1,1] and renormalizing back to native range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(x):\n",
    "    a = np.min(x)\n",
    "    b = np.max(x)\n",
    "    mu = (a+b)/2.0\n",
    "    sigma = (b-a)/2.0\n",
    "    return [mu, sigma]\n",
    "\n",
    "def normalise(x, mu, sigma):\n",
    "    return (x-mu)/sigma\n",
    "\n",
    "def renormalise(x, mu, sigma):\n",
    "    return x*sigma + mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare waveform for net training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the proper MSE loss based on the datatype\n",
    "mse = torch.nn.MSELoss().type(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN(nn.Module):\n",
    "    def __init__(self, nz, ngf=64, output_size=1024, nc=1, num_measurements=64):\n",
    "        super(DCGAN, self).__init__()\n",
    "        self.nc = nc\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Deconv Layers: (in_channels, out_channels, kernel_size, stride, padding, bias = false)\n",
    "        # Inputs: R^(N x Cin x Lin), Outputs: R^(N, Cout, Lout) s.t. Lout = (Lin - 1)*stride - 2*padding + kernel_size\n",
    "\n",
    "        self.conv1 = nn.ConvTranspose1d(nz, ngf, 4, 1, 0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(ngf)\n",
    "        # LAYER 1: input: (random) zϵR^(nzx1), output: x1ϵR^(64x4) (channels x length)\n",
    "\n",
    "        self.conv2 = nn.ConvTranspose1d(ngf, ngf, 6, 2, 2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(ngf)\n",
    "        # LAYER 2: input: x1ϵR^(64x4), output: x2ϵR^(64x8) (channels x length)\n",
    "\n",
    "        self.conv3 = nn.ConvTranspose1d(ngf, ngf, 6, 2, 2, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(ngf)\n",
    "        # LAYER 3: input: x1ϵR^(64x8), output: x2ϵR^(64x16) (channels x length)\n",
    "\n",
    "        self.conv4 = nn.ConvTranspose1d(ngf, ngf, 6, 2, 2, bias=False)\n",
    "        self.bn4 = nn.BatchNorm1d(ngf)\n",
    "        # LAYER 4: input: x1ϵR^(64x16), output: x2ϵR^(64x32) (channels x length)\n",
    "\n",
    "        self.conv5 = nn.ConvTranspose1d(ngf, ngf, 6, 2, 2, bias=False)\n",
    "        self.bn5 = nn.BatchNorm1d(ngf)\n",
    "        # LAYER 5: input: x2ϵR^(64x32), output: x3ϵR^(64x64) (channels x length)\n",
    "\n",
    "        self.conv6 = nn.ConvTranspose1d(ngf, ngf, 6, 2, 2, bias=False)\n",
    "        self.bn6 = nn.BatchNorm1d(ngf)\n",
    "        # LAYER 6: input: x3ϵR^(64x64), output: x4ϵR^(64x128) (channels x length)\n",
    "\n",
    "        self.conv7 = nn.ConvTranspose1d(ngf, ngf, 6, 2, 2, bias=False)\n",
    "        self.bn7 = nn.BatchNorm1d(ngf)\n",
    "        # LAYER 7: input: x4ϵR^(64x128), output: x5ϵR^(64x256) (channels x length)\n",
    "\n",
    "        self.conv8 = nn.ConvTranspose1d(ngf, ngf, 6, 2, 2, bias=False)\n",
    "        self.bn8 = nn.BatchNorm1d(ngf)\n",
    "        # LAYER 8: input: x5ϵR^(64x256), output: x6ϵR^(64x512) (channels x length)\n",
    "\n",
    "        self.conv9 = nn.ConvTranspose1d(ngf, nc, 4, 2, 1, bias=False)  # output is image\n",
    "        # LAYER 9: input: x6ϵR^(64x512), output: (sinusoid) G(z,w)ϵR^(1x1024) (channels x length)\n",
    "\n",
    "        self.fc = nn.Linear(output_size * nc, num_measurements, bias=False)  # output is A; measurement matrix\n",
    "        # each entry should be drawn from a Gaussian (random noisy measurements)\n",
    "        # don't compute gradient of self.fc! memory issues\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_size = x.size()\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = F.relu(self.bn6(self.conv6(x)))\n",
    "        x = F.relu(self.bn7(self.conv7(x)))\n",
    "        x = F.relu(self.bn8(self.conv8(x)))\n",
    "        x = F.tanh(self.conv9(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def measurements(self, x):\n",
    "        # this gives the image - make it a single row vector of appropriate length\n",
    "        y = self.forward(x).view(1, -1)\n",
    "        y = y.cpu()\n",
    "\n",
    "        # pass thru FC layer - returns A*image\n",
    "        meas = self.fc(y)\n",
    "\n",
    "        if CUDA:\n",
    "            return meas.cuda()\n",
    "        else:\n",
    "            return meas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network while tracking loss vs. reference wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sravula/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "num_samples = 20\n",
    "period_list = [1, 4, 16, 64, 128, 256, 512]\n",
    "\n",
    "mse_log = (1e6)*np.ones((len(period_list), num_samples))\n",
    "iter_log = (1e6)*np.ones((len(period_list), num_samples))\n",
    "\n",
    "for pd in range(len(period_list)):\n",
    "    print(pd)\n",
    "    for j in range(num_samples):\n",
    "        \n",
    "        # get a DCGAN that outputs images of size WAVE_SIZE\n",
    "        net = DCGAN(Z_NUM,NGF,WAVE_SIZE,nc,num_measurements) # initialize network\n",
    "        net.fc.requires_grad = False\n",
    "\n",
    "        if CUDA: # move network to GPU if available\n",
    "            net.cuda()\n",
    "\n",
    "        if compressed:\n",
    "            net.fc.weight.data = (1/math.sqrt(1.0*num_measurements)) * torch.randn(num_measurements, WAVE_SIZE*nc)\n",
    "        else:\n",
    "            net.fc.weight.data = torch.eye(num_measurements)\n",
    "\n",
    "        allparams = [x for x in net.parameters()] #specifies which to compute gradients of\n",
    "        allparams = allparams[:-1] # get rid of last item in list (fc layer) because it's memory intensive\n",
    "\n",
    "        z = Variable(torch.zeros(ALEX_BATCH_SIZE*Z_NUM).type(dtype).view(ALEX_BATCH_SIZE,Z_NUM,1))\n",
    "        z.data.normal_().type(dtype)\n",
    "\n",
    "        # Define optimizer\n",
    "        optim = torch.optim.RMSprop(allparams,lr=LR,momentum=MOM, weight_decay=WD)\n",
    "        \n",
    "        y0 = get_sinusoid(num_samples = WAVE_SIZE, num_periods = period_list[pd], noisy=True)\n",
    "        y0_denoised = get_sinusoid(num_samples = WAVE_SIZE, num_periods = period_list[pd], noisy=False)\n",
    "        \n",
    "        MU = get_stats(y0)[0]\n",
    "        SIGMA = get_stats(y0)[1]\n",
    "\n",
    "        y = torch.Tensor(y0)\n",
    "        y = normalise(y, MU, SIGMA)\n",
    "        y = Variable(y.type(dtype))\n",
    "        \n",
    "        measurements = Variable(torch.mm(y.cpu().data.view(ALEX_BATCH_SIZE,-1),net.fc.weight.data.permute(1,0)),requires_grad=False) \n",
    "\n",
    "        if CUDA: # move measurements to GPU if possible\n",
    "            measurements = measurements.cuda()\n",
    "        \n",
    "        for i in range(NUM_ITER):\n",
    "            optim.zero_grad() # clears graidents of all optimized variables\n",
    "            out = net(z) # produces wave (in form of data tensor) i.e. G(z,w)\n",
    "    \n",
    "            loss = mse(net.measurements(z),measurements) # calculate loss between AG(z,w) and Ay\n",
    "         \n",
    "            # DCGAN output is in [-1,1]. Renormalise to [0,1] before plotting\n",
    "            wave = renormalise(out, MU, SIGMA).data[0].cpu().numpy()[0,:] \n",
    "\n",
    "            cur_mse = np.mean((y0_denoised - wave)**2)\n",
    "            \n",
    "            if (cur_mse <= mse_log[pd][j]):\n",
    "                mse_log[pd][j] = cur_mse\n",
    "                iter_log[pd][j] = i\n",
    "    \n",
    "            loss.backward()\n",
    "            optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[68. 70. 52. 50. 71. 66. 86. 71. 77. 81. 56. 94. 58. 59. 82. 67. 77. 65.\n",
      "  57. 66.]\n",
      " [67. 43. 47. 55. 74. 52. 56. 67. 62. 88. 74. 70. 59. 47. 47. 51. 44. 65.\n",
      "  57. 60.]\n",
      " [67. 41. 83. 42. 71. 80. 52. 53. 52. 67. 48. 61. 71. 54. 71. 60. 50. 69.\n",
      "  55. 67.]\n",
      " [60. 44. 60. 82. 67. 66. 56. 58. 70. 71. 49. 69. 47. 67. 81. 63. 52. 65.\n",
      "  68. 62.]\n",
      " [44. 50. 52. 44. 45. 65. 43. 45. 52. 32. 54. 70. 58. 59. 55. 47. 70. 60.\n",
      "  60. 44.]\n",
      " [33. 36. 48. 27. 55. 95. 41. 75. 46. 43. 34. 38. 48. 52. 61. 48. 49. 44.\n",
      "  45. 47.]\n",
      " [18. 15.  7.  9. 18.  6.  6. 22. 13.  9. 21. 12. 22. 15. 13.  8. 11.  6.\n",
      "  15. 11.]]\n"
     ]
    }
   ],
   "source": [
    "print(iter_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mse = np.around(np.mean(mse_log, axis = 1), 5)\n",
    "mean_iter = np.around(np.mean(iter_log, axis = 1), 1)\n",
    "std_iter = np.around(np.std(iter_log, axis = 1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Periodicities:  [1, 4, 16, 64, 128, 256, 512]\n",
      "Mean MSE:  [0.00101 0.00103 0.00093 0.00071 0.00086 0.00108 0.00049]\n",
      "Mean best iteration:  [68.6 59.2 60.7 62.8 52.4 48.2 12.8]\n",
      "STD best iteration:  [11.4 11.5 11.6  9.8  9.6 14.8  5.2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Periodicities: \", period_list)\n",
    "print(\"Mean MSE: \", mean_mse)\n",
    "print(\"Mean best iteration: \", mean_iter)\n",
    "print(\"STD best iteration: \", std_iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
